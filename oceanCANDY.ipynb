{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "import h5py\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "# set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Define hyperparameters\n",
    "num_clusters = 2\n",
    "learning_rate = 0.0005\n",
    "batch_size = 1\n",
    "num_epochs = 10\n",
    "input_size = 496\n",
    "hidden_size = 792\n",
    "num_epochs = 30\n",
    "\n",
    "# load data from mat file\n",
    "with h5py.File(\n",
    "    \"your_document_path\",\n",
    "    \"r\",\n",
    ") as f:\n",
    "    samples = f[\"samples\"][()]\n",
    "    labels = f[\"y_m_d_jd_mjd_table\"][3, :]  # extract day column from labels、\n",
    "    samples = samples[:int(samples.shape[0]/4), :, :]\n",
    "    labels = labels[:int(samples.shape[0]/4)]\n",
    "\n",
    "# define window size and stride\n",
    "window_size = 14\n",
    "stride = 1\n",
    "\n",
    "# preprocess data into input/output pairs\n",
    "inputs = []\n",
    "outputs = []\n",
    "for i in range(window_size, samples.shape[0] - 1, stride):\n",
    "    x = samples[i - window_size : i, :, :]\n",
    "    y = samples[i + 1, :, :]\n",
    "    inputs.append(x)\n",
    "    outputs.append(y)\n",
    "\n",
    "\n",
    "# input shape: 14 * 264 * 496\n",
    "inputs = torch.stack([torch.from_numpy(x) for x in inputs])\n",
    "outputs = torch.stack([torch.from_numpy(y) for y in outputs])\n",
    "\n",
    "# split data into train and test sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    inputs, outputs, test_size=0.2, random_state=42\n",
    ")\n",
    "x_train = torch.Tensor(x_train).to(device)\n",
    "y_train = torch.Tensor(y_train).to(device)\n",
    "train_dataset = TensorDataset(x_train, y_train)\n",
    "x_test = torch.Tensor(x_test).to(device)\n",
    "y_test = torch.Tensor(y_test).to(device)\n",
    "test_dataset = TensorDataset(x_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kmeans method to split p/z sets\n",
    "\n",
    "class CANDY_Kmeans(nn.Module):\n",
    "    def __init__(self, input_size, num_clusters, hidden_size):\n",
    "        super(CANDY_Kmeans, self).__init__()\n",
    "        \n",
    "        self.fc0 = nn.Linear(14, 3).double()\n",
    "\n",
    "        # k-means clustering to split input into p/z sets\n",
    "        self.num_clusters = num_clusters\n",
    "        self.kmeans = KMeans(n_clusters=num_clusters, n_init=\"auto\")\n",
    "\n",
    "        # layer 1: input layer\n",
    "        self.input_layer = nn.Linear(input_size, input_size).double()\n",
    "\n",
    "        # layer 2: p set output\n",
    "        self.p_output_layer = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size), nn.ReLU(True)\n",
    "        ).double()\n",
    "        self.Wp = nn.Parameter(\n",
    "            torch.tril(torch.randn(hidden_size, hidden_size)), requires_grad=True\n",
    "        ).double()\n",
    "        self.Wp.data.diagonal().fill_(1)\n",
    "        self.Wp_diag = nn.Parameter(\n",
    "            torch.ones(hidden_size), requires_grad=True\n",
    "        ).double()\n",
    "\n",
    "        # layer 3: z set output\n",
    "        self.z_output_layer = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size), nn.ReLU(True)\n",
    "        ).double()\n",
    "        self.Wz = nn.Parameter(\n",
    "            torch.randn(hidden_size, hidden_size), requires_grad=True\n",
    "        ).double()\n",
    "\n",
    "        # layer 4: fully connected output\n",
    "        self.fc1 = nn.Linear(3, 1).double()\n",
    "\n",
    "        # MLP layer\n",
    "        self.fc2 = nn.Sequential(\n",
    "            nn.Linear(input_size, input_size),nn.ReLU(True)\n",
    "        ).double()\n",
    "        self.fc3 = nn.Linear(input_size, input_size).double()\n",
    "        self.mlp_layer = nn.Sequential(\n",
    "            nn.Linear(input_size, input_size), nn.BatchNorm1d(input_size), nn.ReLU(True)\n",
    "        ).double()\n",
    "\n",
    "    def kmeans_cluster(self, x):\n",
    "        \n",
    "        # k-means clustering to split input into p/z sets\n",
    "        x = x.reshape(x.size(-1), -1)\n",
    "        self.kmeans.fit_predict(x.cpu().detach().numpy())\n",
    "\n",
    "        # Create boolean tensors indicating the positions where the labels are 0 or 1\n",
    "        p_mask = self.kmeans.labels_ == 0\n",
    "        z_mask = self.kmeans.labels_ == 1\n",
    "\n",
    "        # Create p_set tensor with values from x where p_mask is True, and 0 otherwise\n",
    "        p_set = torch.zeros_like(x)\n",
    "        p_set[p_mask] = x[p_mask]\n",
    "\n",
    "        # Create z_set tensor with values from x where z_mask is True, and 0 otherwise\n",
    "        z_set = torch.zeros_like(x)\n",
    "        z_set[z_mask] = x[z_mask]\n",
    "\n",
    "        return p_set.double(), z_set.double()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = x.contiguous().view(264, 496, 14)\n",
    "        x = self.fc0(x)\n",
    "        x = x.view(3, 264, 496)\n",
    "\n",
    "        # perform k-means clustering to split input into p/z sets\n",
    "        p_set, z_set = self.kmeans_cluster(x)\n",
    "\n",
    "        p_output = self.p_output_layer(p_set)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # convert Wp to lower triangular matrix\n",
    "            self.Wp.data = torch.tril(self.Wp.data)\n",
    "            self.Wp.data.diagonal().clamp_(min=0, max=1)\n",
    "        Wp = self.Wp.double() + torch.diag(self.Wp_diag)\n",
    "        p_output = p_output.to(device)\n",
    "        Wp = Wp.to(device)\n",
    "        p_output = p_output.view(p_output.shape[-1], -1)\n",
    "        p_output = torch.mm(Wp, p_output) \n",
    "\n",
    "        # layer 3: z set output\n",
    "        z_output = self.z_output_layer(z_set)\n",
    "        z_output = z_output.view(z_output.shape[-1], -1).to(device)\n",
    "        Wz = self.Wz.double().to(device)\n",
    "        z_output = torch.mm(Wz, p_output)\n",
    "        p_output = p_output.view(3, 1, 264, 496)\n",
    "        z_output = z_output.view(3, 1, 264, 496)\n",
    "\n",
    "        # layer 4: fully connected output\n",
    "        combined_output = p_output + z_output\n",
    "        output = combined_output.view(-1, 3).to(device)\n",
    "        output = self.fc1(output)\n",
    "        \n",
    "        # new layer: MLP layer\n",
    "        output = output.view(1, 264, 496).to(device)\n",
    "        output = self.fc2(output)\n",
    "        output = self.fc3(output)\n",
    "        output = output.view(1, 264, 496)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "# Define the model\n",
    "model = CANDY_Kmeans(input_size, num_clusters, hidden_size).to(device)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "train_losses = []\n",
    "test_losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the map of p/z partition\n",
    "\n",
    "app_data = samples[-20:, :, :]\n",
    "app_data = torch.tensor(app_data, dtype=torch.float32).to(device)\n",
    "inputs = app_data[:14, :, :].double()\n",
    "inputs = inputs.contiguous().view(264, 496, 14)\n",
    "input1 = model.fc0(inputs)\n",
    "input1 = input1.view(3, 264, 496)\n",
    "\n",
    "p_set, z_set = model.kmeans_cluster(input1)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "extent = [180, 120, -10, 10]\n",
    "heatmap = ax.imshow(p_set[:, :264].cpu().detach().numpy(),vmin= -1, vmax = 1, cmap='rainbow', extent=extent)\n",
    "ax.set_xticks(np.linspace(120, 180, 5))\n",
    "ax.set_yticks(np.linspace(-10, 10, 5))\n",
    "ax.set_xticklabels([f\"{int(label)}°{'W' if label < 0 else 'E'}\" for label in np.linspace(120, 180, 5)])\n",
    "ax.set_yticklabels([f\"{int(label)}°{'S' if label < 0 else 'N'}\" for label in np.linspace(-10, 10, 5)])\n",
    "cax = fig.add_axes([0.1, 0.2, 0.8, 0.04])  # [left, bottom, width, height]\n",
    "cbar = plt.colorbar(heatmap, cax=cax, orientation='horizontal', pad=-5)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainable p set method\n",
    "\n",
    "class CANDY_train(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(CANDY_train, self).__init__()\n",
    "\n",
    "        # layer 1: input layer\n",
    "        self.fc0 = nn.Linear(14, 3).double()\n",
    "\n",
    "        # Initialize the p_mask hyperparameter\n",
    "        self.p_mask = nn.Parameter(torch.randn(input_size), requires_grad=True).double()\n",
    "\n",
    "        # layer 2: p set output\n",
    "        self.p_output_layer = nn.Sequential(\n",
    "            nn.Linear(input_size, input_size),\n",
    "            nn.ReLU(True)\n",
    "        ).double()\n",
    "        self.Wp = nn.Parameter(\n",
    "            torch.tril(torch.randn(input_size, input_size)), requires_grad=True\n",
    "        ).double()\n",
    "        self.Wp.data.diagonal().fill_(1)\n",
    "        self.Wp_diag = nn.Parameter(\n",
    "            torch.ones(input_size), requires_grad=True\n",
    "        ).double()\n",
    "\n",
    "        # layer 3: z set output\n",
    "        self.z_output_layer = nn.Sequential(\n",
    "            nn.Linear(input_size, input_size),\n",
    "            nn.ReLU(True)\n",
    "        ).double()\n",
    "        self.Wz = nn.Parameter(\n",
    "            torch.randn(input_size, input_size), requires_grad=True\n",
    "        ).double()\n",
    "\n",
    "        # layer 4: fully connected output\n",
    "        self.fc1 = nn.Linear(3, 1).double()\n",
    "\n",
    "        # MLP layer\n",
    "        self.fc2 = nn.Sequential(\n",
    "            nn.Linear(input_size, input_size),\n",
    "            nn.ReLU(True)\n",
    "        ).double()\n",
    "        self.fc3 = nn.Linear(input_size, input_size).double()\n",
    "\n",
    "    def split_input(self, x):\n",
    "        # Use the p_mask to split the input into p and z sets\n",
    "        p_mask = self.p_mask.to(x.device) > 0\n",
    "        z_mask = ~p_mask\n",
    "\n",
    "        p_set = x * p_mask\n",
    "        z_set = x * z_mask.to(x.device)\n",
    "\n",
    "        return p_set.double(), z_set.double()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.contiguous().view(264, 496, 14)\n",
    "        x = self.fc0(x)\n",
    "        x = x.view(3, 264, 496)\n",
    "\n",
    "        # Split input into p and z sets\n",
    "        p_set, z_set = self.split_input(x)\n",
    "        \n",
    "        p_output = self.p_output_layer(p_set)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            self.Wp.data = torch.tril(self.Wp.data)\n",
    "            self.Wp.data.diagonal().clamp_(min=0, max=1)\n",
    "        Wp = self.Wp.double() + torch.diag(self.Wp_diag)\n",
    "        p_output = p_output.to(device)\n",
    "        Wp = Wp.to(device)\n",
    "        p_output = p_output.view(p_output.shape[-1], -1)\n",
    "        p_output = torch.mm(Wp, p_output)\n",
    "\n",
    "        z_output = self.z_output_layer(z_set)\n",
    "        z_output = z_output.view(z_output.shape[-1], -1).to(device)\n",
    "        Wz = self.Wz.double().to(device)\n",
    "        z_output = torch.mm(Wz, p_output)\n",
    "        p_output = p_output.view(3, 264, 496)\n",
    "        z_output = z_output.view(3, 264, 496)\n",
    "\n",
    "        combined_output = p_output + z_output\n",
    "        output = combined_output.view(-1, 3).to(device)\n",
    "        output = self.fc1(output)\n",
    "\n",
    "        output = output.view(1, 264, 496).to(device)\n",
    "        output = self.fc2(output)\n",
    "        output = self.fc3(output)\n",
    "        output = output.view(1, 264, 496)\n",
    "\n",
    "        return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fixed p set method\n",
    "\n",
    "class CANDY_fixed(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(CANDY_fixed, self).__init__()\n",
    "\n",
    "        # layer 1: input layer\n",
    "        self.fc0 = nn.Linear(14, 3).double()\n",
    "\n",
    "        # Initialize the p_mask hyperparameter\n",
    "        p_mask = torch.zeros(input_size)\n",
    "        p_mask[::2] = 1  # Set every other element to 1, creating a chess grid\n",
    "        self.p_mask = nn.Parameter(p_mask, requires_grad=False).double()\n",
    "        \n",
    "        # layer 2: p set output\n",
    "        self.p_output_layer = nn.Sequential(\n",
    "            nn.Linear(input_size, input_size),\n",
    "            nn.ReLU(True)\n",
    "        ).double()\n",
    "        self.Wp = nn.Parameter(\n",
    "            torch.tril(torch.randn(input_size, input_size)), requires_grad=True\n",
    "        ).double()\n",
    "        self.Wp.data.diagonal().fill_(1)\n",
    "        self.Wp_diag = nn.Parameter(\n",
    "            torch.ones(input_size), requires_grad=True\n",
    "        ).double()\n",
    "\n",
    "        # layer 3: z set output\n",
    "        self.z_output_layer = nn.Sequential(\n",
    "            nn.Linear(input_size, input_size),\n",
    "            nn.ReLU(True)\n",
    "        ).double()\n",
    "        self.Wz = nn.Parameter(\n",
    "            torch.randn(input_size, input_size), requires_grad=True\n",
    "        ).double()\n",
    "\n",
    "        # layer 4: fully connected output\n",
    "        self.fc1 = nn.Linear(3, 1).double()\n",
    "\n",
    "        # MLP layer\n",
    "        self.fc2 = nn.Sequential(\n",
    "            nn.Linear(input_size, input_size),\n",
    "            nn.ReLU(True)\n",
    "        ).double()\n",
    "        self.fc3 = nn.Linear(input_size, input_size).double()\n",
    "\n",
    "    def split_input(self, x):\n",
    "        # Use the p_mask to split the input into p and z sets\n",
    "        p_mask = self.p_mask.to(x.device) > 0\n",
    "        z_mask = ~p_mask\n",
    "\n",
    "        p_set = x * p_mask\n",
    "        z_set = x * z_mask.to(x.device)\n",
    "\n",
    "        return p_set.double(), z_set.double()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.contiguous().view(264, 496, 14)\n",
    "        x = self.fc0(x)\n",
    "        x = x.view(3, 264, 496)\n",
    "\n",
    "        # Split input into p and z sets\n",
    "        p_set, z_set = self.split_input(x)\n",
    "        \n",
    "        p_output = self.p_output_layer(p_set)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            self.Wp.data = torch.tril(self.Wp.data)\n",
    "            self.Wp.data.diagonal().clamp_(min=0, max=1)\n",
    "        Wp = self.Wp.double() + torch.diag(self.Wp_diag)\n",
    "        p_output = p_output.to(device)\n",
    "        Wp = Wp.to(device)\n",
    "        p_output = p_output.view(p_output.shape[-1], -1)\n",
    "        p_output = torch.mm(Wp, p_output)\n",
    "\n",
    "        z_output = self.z_output_layer(z_set)\n",
    "        z_output = z_output.view(z_output.shape[-1], -1).to(device)\n",
    "        Wz = self.Wz.double().to(device)\n",
    "        z_output = torch.mm(Wz, p_output)\n",
    "        p_output = p_output.view(3, 264, 496)\n",
    "        z_output = z_output.view(3, 264, 496)\n",
    "\n",
    "        combined_output = p_output + z_output\n",
    "        output = combined_output.view(-1, 3).to(device)\n",
    "        output = self.fc1(output)\n",
    "\n",
    "        output = output.view(1, 264, 496).to(device)\n",
    "        output = self.fc2(output)\n",
    "        output = self.fc3(output)\n",
    "        output = output.view(1, 264, 496)\n",
    "\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# layer adjusted method\n",
    "\n",
    "class CANDY_layer_adjusted(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(CANDY_layer_adjusted, self).__init__()\n",
    "\n",
    "        # layer 1: input layer\n",
    "        self.fc0 = nn.Linear(14, 3).double()\n",
    "\n",
    "        # Initialize the p_mask hyperparameter\n",
    "        self.p_mask = nn.Parameter(torch.randn(input_size), requires_grad=True).double()\n",
    "\n",
    "        # layer 2: p set output\n",
    "        self.p_output_layer = nn.Sequential(\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(input_size, input_size),\n",
    "            nn.ReLU(True)\n",
    "        ).double()\n",
    "        self.Wp = nn.Parameter(\n",
    "            torch.tril(torch.randn(hidden_size, hidden_size)), requires_grad=True\n",
    "        ).double()\n",
    "        self.Wp.data.diagonal().fill_(1)\n",
    "        self.Wp_diag = nn.Parameter(\n",
    "            torch.ones(hidden_size), requires_grad=True\n",
    "        ).double()\n",
    "\n",
    "        # layer 3: z set output\n",
    "        self.z_output_layer = nn.Sequential(\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(input_size, input_size),\n",
    "            nn.ReLU(True)\n",
    "        ).double()\n",
    "        self.Wz = nn.Parameter(\n",
    "            torch.randn(hidden_size, hidden_size), requires_grad=True\n",
    "        ).double()\n",
    "\n",
    "        # layer 4: fully connected output\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Linear(3, 1),\n",
    "            nn.ReLU(True)).double()\n",
    "\n",
    "        # MLP layer\n",
    "        self.fc2 = nn.Sequential(\n",
    "            nn.Linear(input_size, input_size),\n",
    "            nn.ReLU(True)\n",
    "        ).double()\n",
    "        self.fc3 = nn.Linear(input_size, input_size).double()\n",
    "\n",
    "    def split_input(self, x):\n",
    "        # Use the p_mask to split the input into p and z sets\n",
    "        p_mask = self.p_mask.to(x.device) > 0\n",
    "\n",
    "        p_set = x * p_mask\n",
    "\n",
    "        return p_set.double()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.contiguous().view(264, 496, 14)\n",
    "        x = self.fc0(x)\n",
    "        x = x.view(3*264, 496)\n",
    "\n",
    "        p_set = self.split_input(x)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # convert Wp to lower triangular matrix\n",
    "            self.Wp.data = torch.tril(self.Wp.data)\n",
    "            self.Wp.data.diagonal().clamp_(min=0, max=1)\n",
    "        Wp = self.Wp.double() + torch.diag(self.Wp_diag)\n",
    "        Wp = Wp.to(device)\n",
    "        p_output = torch.mm(Wp, p_set)\n",
    "        p_output = self.p_output_layer(p_output)\n",
    "\n",
    "        # layer 3: z set output\n",
    "        Wz = self.Wz.double().to(device)\n",
    "        z_output = torch.mm(Wz, p_output)\n",
    "        z_output = self.z_output_layer(z_output)\n",
    "        p_output = p_output.view(3, 264, 496)\n",
    "        z_output = z_output.view(3, 264, 496)\n",
    "\n",
    "        combined_output = p_output + z_output\n",
    "        output = combined_output.view(264, 496, 3).to(device)\n",
    "        output = self.fc1(output)\n",
    "\n",
    "        output = output.view(1, 264, 496).to(device)\n",
    "        # output = self.fc2(output)\n",
    "        output = self.fc3(output)\n",
    "        output = output.view(1, 264, 496)\n",
    "\n",
    "        return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define train and test function\n",
    "\n",
    "def train(model, train_loader, optimizer, criterion):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        inputs, pred = data\n",
    "        inputs = inputs.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = 0\n",
    "        loss += criterion(outputs, pred)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "\n",
    "    train_loss = running_loss / len(train_loader)\n",
    "    train_losses.append(train_loss)\n",
    "    return train_loss\n",
    "\n",
    "\n",
    "def test(model, test_loader, criterion):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(test_loader, 0):\n",
    "            inputs, pred = data\n",
    "            inputs = inputs.to(device)\n",
    "\n",
    "            batch_size = inputs.size(0)\n",
    "            outputs = []\n",
    "\n",
    "            for j in range(batch_size):\n",
    "                input_batch = inputs[j].unsqueeze(0)\n",
    "                output_batch = model(input_batch)\n",
    "                outputs.append(output_batch)\n",
    "\n",
    "                loss = 0\n",
    "                loss = criterion(\n",
    "                    output_batch, pred\n",
    "                )\n",
    "\n",
    "                running_loss += loss.item()\n",
    "\n",
    "            outputs = torch.cat(outputs, dim=0)\n",
    "\n",
    "    test_loss = running_loss / len(test_loader)\n",
    "    test_losses.append(test_loss)\n",
    "    return test_loss\n",
    "\n",
    "\n",
    "epoch_times = []\n",
    "for epoch in range(num_epochs):\n",
    "    start_time = time.time() \n",
    "    train_loss = train(model, train_loader, optimizer, criterion)\n",
    "    test_loss = test(model, test_loader, criterion)\n",
    "    end_time = time.time()\n",
    "    epoch_time = end_time - start_time\n",
    "    epoch_times.append(epoch_time)\n",
    "    print(\n",
    "        f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.6f}, Test Loss: {test_loss:.6f}, Epoch Time: {epoch_time:.2f} seconds\"\n",
    "    )\n",
    "\n",
    "test_loss = test(model, test_loader, criterion)\n",
    "print(f\"Test Loss: {test_loss:.6f}\")\n",
    "\n",
    "total_training_time = sum(epoch_times)\n",
    "print(f\"Total Training Time: {total_training_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define app function\n",
    "\n",
    "input_total = 496\n",
    "app_loss = 0\n",
    "\n",
    "def app(model):\n",
    "    # load application data\n",
    "    app_data = samples[:20, :, :]\n",
    "    app_data = torch.tensor(app_data, dtype=torch.float32).to(device)\n",
    "    # make predictions\n",
    "    inputs = app_data[:14, :, :]\n",
    "    for i in range(3):\n",
    "        app_loss = 0\n",
    "        x = inputs.double()       \n",
    "        y_pred = model(x)\n",
    "        app_loss += criterion(y_pred, app_data[15+i, :, :])         \n",
    "        print(app_loss)  \n",
    "        \n",
    "        # calculate RMSE\n",
    "        rmse = torch.sqrt(torch.mean((y_pred[:, :, :].unsqueeze(0) - app_data[15+i, :, :]) ** 2)).item()\n",
    "        print(\"RMSE\", i+1, \":\", rmse)\n",
    "        inputs = torch.cat((inputs[1:, :, :], y_pred), axis=0).detach()\n",
    "        y_pred = y_pred.cpu().detach().numpy()\n",
    "\n",
    "        fig, ax = plt.subplots()\n",
    "        extent = [180, 120, -10, 10]\n",
    "        heatmap = ax.imshow(y_pred[0, :, :], vmin=22, vmax=30, cmap='jet', extent=extent)\n",
    "        ax.set_xticks(np.linspace(120, 180, 5))\n",
    "        ax.set_yticks(np.linspace(-10, 10, 5))\n",
    "        ax.set_xticklabels([f\"{int(label)}°{'W' if label < 0 else 'E'}\" for label in np.linspace(120, 180, 5)])\n",
    "        ax.set_yticklabels([f\"{int(label)}°{'S' if label < 0 else 'N'}\" for label in np.linspace(-10, 10, 5)])\n",
    "        cax = fig.add_axes([0.1, 0.2, 0.8, 0.04])  # [left, bottom, width, height]\n",
    "        cbar = plt.colorbar(heatmap, cax=cax, orientation='horizontal', pad=-5)\n",
    "        plt.show()\n",
    "\n",
    "        fig, ax = plt.subplots()\n",
    "        extent = [180, 120, -10, 10]\n",
    "        heatmap = ax.imshow(app_data[15+i, :, :].cpu().detach().numpy(), vmin=22, vmax=30, cmap='jet', extent=extent)\n",
    "        ax.set_xticks(np.linspace(120, 180, 5))\n",
    "        ax.set_yticks(np.linspace(-10, 10, 5))\n",
    "        ax.set_xticklabels([f\"{int(label)}°{'W' if label < 0 else 'E'}\" for label in np.linspace(120, 180, 5)])\n",
    "        ax.set_yticklabels([f\"{int(label)}°{'S' if label < 0 else 'N'}\" for label in np.linspace(-10, 10, 5)])\n",
    "        cax = fig.add_axes([0.1, 0.2, 0.8, 0.04])  # [left, bottom, width, height]\n",
    "        cbar = plt.colorbar(heatmap, cax=cax, orientation='horizontal', pad=-5)\n",
    "        plt.show()\n",
    "\n",
    "        fig, ax = plt.subplots()\n",
    "        extent = [180, 120, -10, 10]\n",
    "        heatmap = ax.imshow(y_pred[0, :, :]-app_data[15+i, :, :].cpu().detach().numpy(), vmin=-2, vmax=2, cmap='jet', extent=extent)\n",
    "        ax.set_xticks(np.linspace(120, 180, 5))\n",
    "        ax.set_yticks(np.linspace(-10, 10, 5))\n",
    "        ax.set_xticklabels([f\"{int(label)}°{'W' if label < 0 else 'E'}\" for label in np.linspace(120, 180, 5)])\n",
    "        ax.set_yticklabels([f\"{int(label)}°{'S' if label < 0 else 'N'}\" for label in np.linspace(-10, 10, 5)])\n",
    "        cax = fig.add_axes([0.1, 0.2, 0.8, 0.04])  # [left, bottom, width, height]\n",
    "        cbar = plt.colorbar(heatmap, cax=cax, orientation='horizontal', pad=-5)\n",
    "        plt.show()\n",
    "        \n",
    "\n",
    "    return y_pred\n",
    "\n",
    "\n",
    "y_app = app(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model, loss function and optimizer\n",
    "model = CANDY_train(input_size, hidden_size).to(device)\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "train_losses = []\n",
    "test_losses = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "\n",
    "epoch_times = []\n",
    "for epoch in range(num_epochs):\n",
    "    start_time = time.time() \n",
    "    train_loss = train(model, train_loader, optimizer, criterion)\n",
    "    test_loss = test(model, test_loader, criterion)\n",
    "    end_time = time.time()\n",
    "    epoch_time = end_time - start_time\n",
    "    epoch_times.append(epoch_time)\n",
    "    print(\n",
    "        f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.6f}, Test Loss: {test_loss:.6f}, Epoch Time: {epoch_time:.2f} seconds\"\n",
    "    )\n",
    "\n",
    "test_loss = test(model, test_loader, criterion)\n",
    "print(f\"Test Loss: {test_loss:.6f}\")\n",
    "\n",
    "total_training_time = sum(epoch_times)\n",
    "print(f\"Total Training Time: {total_training_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# app\n",
    "\n",
    "y_app = app(model)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
